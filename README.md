C++ Order Book Performance AnalysisThis project benchmarks the performance of several different C++ implementations of a financial order book. The goal is to explore how different data structures and algorithms affect latency in a high-performance context.The implementations and analysis are inspired by the CppCon 2024 talk "When Nanoseconds Matter: Ultrafast Trading Systems in C++" by David Gross.Project OverviewThis repository contains two main components:C++ Benchmark (main.cpp): A command-line program that simulates a high volume of market data operations (add, modify, delete orders) against five different order book implementations. It measures the latency of each operation in nanoseconds and saves the raw data to .csv files.Python Visualization Script (pyt.py): A script that reads the .csv files generated by the C++ benchmark and produces a grid of histograms to visually compare the latency distributions of each implementation.Implementations BenchmarkedThe project compares five distinct approaches to building an order book, demonstrating the trade-offs between them:std::map: A standard tree-based implementation. It's easy to write but suffers from poor cache locality because its nodes are scattered in memory, leading to slower performance.std::vector (Intuitive Ordering): Uses a sorted std::vector with the best prices at the front. While cache-friendly, it has very high latency spikes when new best prices are added, as this requires shifting all other elements.std::vector (Efficient Ordering): A key optimization where the best prices are kept at the back of the vector. This turns most top-of-book insertions into fast push_back-like operations, eliminating the costly element shifting.Branchless Binary Search: An optimization on the efficient vector that replaces conditional if/else branches in the binary search with arithmetic. This reduces CPU pipeline stalls from branch mispredictions.Linear Search (The Winner): The fastest implementation for this use case. Although theoretically slower (O(N)), its perfectly sequential memory access pattern works in harmony with the CPU's prefetcher, making it faster in practice than the "smarter" but less cache-friendly binary search.How to RunFollow these two steps to replicate the benchmark and generate the performance graphs.Step 1: Run the C++ BenchmarkFirst, you need to compile and run the C++ program. This will create the raw latency data files.# Compile the C++ code (using g++ as an example)
g++ -std=c++17 -O3 -o benchmark main.cpp

# Run the benchmark executable
./benchmark
After this step, you will have six .csv files in your directory (e.g., map_latencies.csv, linear_search_latencies.csv, etc.).Step 2: Generate the VisualizationNext, run the Python script to process the .csv files and create the visual comparison.Prerequisites: Make sure you have the required Python libraries installed.pip install pandas matplotlib scipy
Run the script:python pyt.py
This will generate an image file named latency_distribution_grid_optimized.png containing a 3x2 grid of plots, showing the latency distribution for each of the tested implementations.
